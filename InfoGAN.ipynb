{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmura\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-b9e1941a3cec>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\kmura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\kmura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\kmura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\kmura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../../MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\kmura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)\n",
    "mb_size = 32 \n",
    "Z_dim = 16  # Random noise input for generator\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "h_dim = 128   # Hidden layer dimension\n",
    "cnt = 0\n",
    "lr = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # ##  ==================== GENERATOR ========================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
    "\n",
    "Wzh = xavier_init(size=[Z_dim + 10, h_dim])\n",
    "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whx = xavier_init(size=[h_dim, X_dim])\n",
    "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
    "\n",
    "\n",
    "def G(z, c):\n",
    "    inputs = torch.cat([z, c], 1)\n",
    "    h = nn.relu(inputs @ Wzh + bzh.repeat(inputs.size(0), 1))\n",
    "    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ==================== DISCRIMINATOR ========================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Wxh = xavier_init(size=[X_dim, h_dim])\n",
    "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Why = xavier_init(size=[h_dim, 1])\n",
    "bhy = Variable(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "\n",
    "def D(X):\n",
    "    h = nn.relu(X @ Wxh + bxh.repeat(X.size(0), 1))\n",
    "    y = nn.sigmoid(h @ Why + bhy.repeat(h.size(0), 1))\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================== Q(c|X) ========================== \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Wqxh = xavier_init(size=[X_dim, h_dim])\n",
    "bqxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whc = xavier_init(size=[h_dim, 10])\n",
    "bhc = Variable(torch.zeros(10), requires_grad=True)\n",
    "\n",
    "\n",
    "def Q(X):\n",
    "    h = nn.relu(X @ Wqxh + bqxh.repeat(X.size(0), 1))\n",
    "    c = nn.softmax(h @ Whc + bhc.repeat(h.size(0), 1))\n",
    "    return c\n",
    "\n",
    "\n",
    "G_params = [Wzh, bzh, Whx, bhx]\n",
    "D_params = [Wxh, bxh, Why, bhy]\n",
    "Q_params = [Wqxh, bqxh, Whc, bhc]\n",
    "params = G_params + D_params + Q_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmura\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\kmura\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; D_loss: 1.4468754529953003; G_loss: 2.042823314666748; Idx: 8\n",
      "Iter-1000; D_loss: 0.15183107554912567; G_loss: 4.062690258026123; Idx: 8\n",
      "Iter-2000; D_loss: 0.12419113516807556; G_loss: 3.6387805938720703; Idx: 5\n",
      "Iter-3000; D_loss: 0.15582631528377533; G_loss: 4.37735652923584; Idx: 6\n",
      "Iter-4000; D_loss: 0.14815331995487213; G_loss: 3.676138401031494; Idx: 3\n",
      "Iter-5000; D_loss: 0.508362352848053; G_loss: 4.521950721740723; Idx: 9\n",
      "Iter-6000; D_loss: 0.5160889029502869; G_loss: 2.9797861576080322; Idx: 5\n",
      "Iter-7000; D_loss: 0.4400549530982971; G_loss: 2.8511452674865723; Idx: 7\n",
      "Iter-8000; D_loss: 0.8536098003387451; G_loss: 2.386115312576294; Idx: 5\n",
      "Iter-9000; D_loss: 1.0276927947998047; G_loss: 2.3005568981170654; Idx: 8\n",
      "Iter-10000; D_loss: 0.6924546360969543; G_loss: 1.9490838050842285; Idx: 2\n",
      "Iter-11000; D_loss: 0.5206872224807739; G_loss: 2.314621925354004; Idx: 0\n",
      "Iter-12000; D_loss: 0.5898966789245605; G_loss: 2.147003173828125; Idx: 2\n",
      "Iter-13000; D_loss: 0.6004413962364197; G_loss: 2.390442371368408; Idx: 0\n",
      "Iter-14000; D_loss: 0.6914935111999512; G_loss: 2.0740694999694824; Idx: 1\n",
      "Iter-15000; D_loss: 0.8130288124084473; G_loss: 1.94557523727417; Idx: 6\n",
      "Iter-16000; D_loss: 0.6940081119537354; G_loss: 2.0182018280029297; Idx: 7\n",
      "Iter-17000; D_loss: 0.7969475984573364; G_loss: 1.9446401596069336; Idx: 1\n",
      "Iter-18000; D_loss: 0.9784741401672363; G_loss: 1.5049597024917603; Idx: 2\n",
      "Iter-19000; D_loss: 0.8315763473510742; G_loss: 2.353334665298462; Idx: 8\n",
      "Iter-20000; D_loss: 0.674652099609375; G_loss: 1.9641211032867432; Idx: 8\n",
      "Iter-21000; D_loss: 0.8623499870300293; G_loss: 1.9804936647415161; Idx: 6\n",
      "Iter-22000; D_loss: 0.8295484781265259; G_loss: 2.2931160926818848; Idx: 1\n",
      "Iter-23000; D_loss: 0.8987234234809875; G_loss: 1.3899403810501099; Idx: 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-39b4d8dc1892>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mD_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m## Backpropagate (Calculate gradients)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mD_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m## Update weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# Housekeeping - reset gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kmura\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" ===================== TRAINING ======================== \"\"\"\n",
    "\n",
    "\n",
    "def reset_grad():\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            data = p.grad.data\n",
    "            p.grad = Variable(data.new().resize_as_(data).zero_())\n",
    "\n",
    "\n",
    "G_solver = optim.Adam(G_params, lr=1e-3)\n",
    "D_solver = optim.Adam(D_params, lr=1e-3)\n",
    "Q_solver = optim.Adam(G_params + Q_params, lr=1e-3)\n",
    "\n",
    "\n",
    "### Generate a categorical distribution, with equal probability for each of the ten elements.\n",
    "# Remember that we start with a random categorical distribution [0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ]\n",
    "# At the end, we expect InfoGAN to embed latent contents into categorical distribution that is , categorical representation\n",
    "# for each digit (Like one-hot encoding)\n",
    "\n",
    "def sample_c(size):\n",
    "    c = np.random.multinomial(1, 10*[0.1], size=size)\n",
    "    c = Variable(torch.from_numpy(c.astype('float32')))\n",
    "    return c\n",
    "### We start with sample_c\n",
    "\n",
    "\n",
    "\n",
    "## 100000\n",
    "\n",
    "for it in range(100000):\n",
    "    \n",
    "    \n",
    "    # Sample data\n",
    "    \n",
    "    X, _ = mnist.train.next_batch(mb_size)   ## Get a batch of size 32\n",
    "    X = Variable(torch.from_numpy(X))        ## Convert to torch variable for updating it.    \n",
    "\n",
    "    z = Variable(torch.randn(mb_size, Z_dim)) ## z (32,16) ==> 16-size random vector for each example\n",
    "    c = sample_c(mb_size)                     ## Create uniform categorical distribution for the batch. (32,10) [[0.2,..0.2],[0.2,..,0.2]]  \n",
    "\n",
    "    \n",
    "\n",
    "    # Dicriminator forward-loss-backward-update (Only Train Disc but not generator and Q_c)\n",
    "\n",
    "# ================================== START ==========================================\n",
    "\n",
    "    G_sample = G(z, c)                       ## Generate images given random noise z and prior c.\n",
    "    D_real = D(X)                            ## Make predictions on the actual images.Ideally D_real shud classify it as real.\n",
    "    D_fake = D(G_sample)                     ## Make predictions on the fake (generated) images.\n",
    "\n",
    "    D_loss = -torch.mean(torch.log(D_real + 1e-8) + torch.log(1 - D_fake + 1e-8))\n",
    "    \n",
    "    ## Classify real images as True (1) and fake images as False (0)\n",
    "\n",
    "    D_loss.backward()   ## Backpropagate (Calculate gradients)\n",
    "    D_solver.step()     ## Update weights\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "# ================================= STOP =============================================\n",
    "\n",
    "\n",
    "\n",
    "    # Generator forward-loss-backward-update\n",
    "\n",
    "# ================================== START ==========================================\n",
    "\n",
    "\n",
    "    G_sample = G(z, c)                    ## Generate images given random noise z and prior c.\n",
    "    D_fake = D(G_sample)                  ## Make predictions on the fake (generated) images.\n",
    "\n",
    "    G_loss = -torch.mean(torch.log(D_fake + 1e-8))\n",
    "    \n",
    "    ## Classify fake images as True (1)\n",
    "\n",
    "    G_loss.backward()\n",
    "    G_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "\n",
    "    \n",
    "# ================================== STOP ==========================================\n",
    "\n",
    "\n",
    "    # Q forward-loss-backward-update\n",
    "    \n",
    "# ================================== START ==========================================\n",
    "\n",
    "    G_sample = G(z, c)                    ## Generate images given random noise z and prior c.\n",
    "    Q_c_given_x = Q(G_sample)             ## Generate Q_C (New C)\n",
    "\n",
    "    crossent_loss = torch.mean(-torch.sum(c * torch.log(Q_c_given_x + 1e-8), dim=1))\n",
    "    mi_loss = crossent_loss\n",
    "\n",
    "    mi_loss.backward()\n",
    "    Q_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "\n",
    "# ================================== STOP ==========================================\n",
    "\n",
    "    # Print and plot every now and then\n",
    "    if it % 1000 == 0:\n",
    "        idx = np.random.randint(0, 10)\n",
    "        c = np.zeros([mb_size, 10])\n",
    "        c[range(mb_size), idx] = 1\n",
    "        c = Variable(torch.from_numpy(c.astype('float32')))\n",
    "        samples = G(z, c).data.numpy()[:16]\n",
    "\n",
    "        print('Iter-{}; D_loss: {}; G_loss: {}; Idx: {}'\n",
    "              .format(it, D_loss.data.numpy(), G_loss.data.numpy(), idx))\n",
    "\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "        if not os.path.exists('out/'):\n",
    "            os.makedirs('out/')\n",
    "\n",
    "        plt.savefig('out/{}.png'\n",
    "                    .format(str(cnt).zfill(3)+\"label_\"+str(idx)), bbox_inches='tight')\n",
    "        cnt += 1\n",
    "        plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
